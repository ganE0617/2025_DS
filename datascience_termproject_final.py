# -*- coding: utf-8 -*-
"""DataScience_TermProject_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g0krUZJZY-gmhOHcIPIWgoFLufqlJQIp
"""

from google.colab import drive
drive.mount('/content/drive')

from google.colab import files
import zipfile
import os

# Files upload
uploaded = files.upload()  # upload human+activity+recognition+using+smartphones.zip

# Uncompressed
zip_path = "human+activity+recognition+using+smartphones.zip"
extract_dir = "human_activity_data"

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

# Check internal files
os.listdir(extract_dir)

# Re-check and release internal compressed files
inner_zip = os.path.join(extract_dir, "UCI HAR Dataset.zip")
inner_extract_dir = "UCI_HAR_Dataset"

with zipfile.ZipFile(inner_zip, 'r') as zip_ref:
    zip_ref.extractall(inner_extract_dir)

# Check Dataset Path
os.listdir(os.path.join(inner_extract_dir, "UCI HAR Dataset"))

"""#1. Activity Classification

## 1-1. Feature Reduction
"""

import pandas as pd
import os

# Set the base path
base_path = os.path.join(inner_extract_dir, "UCI HAR Dataset")

# Load feature names
feature_names = pd.read_csv(os.path.join(base_path, "features.txt"),
                            delim_whitespace=True, header=None, names=['index', 'feature'])['feature'].values

# Define function to load a data split
def load_data_split(split="train"):
    X = pd.read_csv(os.path.join(base_path, split, f"X_{split}.txt"),
                    delim_whitespace=True, header=None)
    y = pd.read_csv(os.path.join(base_path, split, f"y_{split}.txt"),
                    delim_whitespace=True, header=None, names=["activity"])
    subjects = pd.read_csv(os.path.join(base_path, split, f"subject_{split}.txt"),
                           delim_whitespace=True, header=None, names=["subject"])

    X.columns = feature_names  # Assign column names
    df = pd.concat([subjects, y, X], axis=1)
    return df

# Load and merge train and test datasets
train_df = load_data_split("train")
test_df = load_data_split("test")
full_df = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)

print(f"Total number of samples: {full_df.shape[0]}")
print(f"Total number of columns: {full_df.shape[1]}")

selected_features = [
    "tBodyAcc-mean()-X", "tBodyAcc-mean()-Y", "tBodyAcc-mean()-Z",
    "tBodyAcc-std()-X", "tBodyAcc-std()-Y", "tBodyAcc-std()-Z",
    "tBodyGyro-mean()-X", "tBodyGyro-mean()-Y", "tBodyGyro-mean()-Z",
    "tGravityAcc-mean()-X", "tGravityAcc-mean()-Y", "tGravityAcc-mean()-Z",
    "tGravityAcc-std()-X", "tGravityAcc-std()-Y", "tGravityAcc-std()-Z",
    "tBodyGyro-entropy()-X", "tBodyGyro-entropy()-Y", "tBodyGyro-entropy()-Z",
    "tBodyAccMag-mean()", "tBodyAccMag-std()",
    "tBodyGyroMag-mean()", "tBodyGyroMag-std()",
    "tBodyAcc-sma()",
    "tBodyAcc-energy()-X", "tBodyAcc-energy()-Y", "tBodyAcc-energy()-Z",
    "tBodyAcc-entropy()-X", "tBodyAcc-entropy()-Y", "tBodyAcc-entropy()-Z",
]

# Extract only the selected features
df_selected = full_df[["subject", "activity"] + selected_features]
df_selected.head()

import matplotlib.pyplot as plt
import seaborn as sns

def plot_feature_distributions_by_activity(df_selected, selected_features, activity_column="activity", max_features_per_fig=6):
    """
    Plot boxplots of each selected feature by activity.

    Parameters:
        df_selected (pd.DataFrame): DataFrame containing activity and selected features
        selected_features (List[str]): List of feature column names to plot
        activity_column (str): Column name containing activity labels
        max_features_per_fig (int): Number of features per figure (for readability)
    """
    total_features = len(selected_features)
    for i in range(0, total_features, max_features_per_fig):
        feature_subset = selected_features[i:i+max_features_per_fig]
        num_cols = len(feature_subset)

        plt.figure(figsize=(5 * num_cols, 5))
        for j, feature in enumerate(feature_subset, 1):
            plt.subplot(1, num_cols, j)
            sns.boxplot(data=df_selected, x=activity_column, y=feature)
            plt.xticks(rotation=45)
            plt.title(feature)
        plt.tight_layout()
        plt.show()

# Run the plot function on df_selected and selected_features
plot_feature_distributions_by_activity(df_selected, selected_features)

"""## 1-2. Data Preprocessing
* Data Claeaning
* Data Scaling (StandardScaler)
* LabelEncoding
"""

import numpy as np
import random

# Function to inject dirty data
def inject_dirty_data(df, dup_ratio=0.01, missing_ratio=0.01):
    df_dirty = df.copy()

    #  1. Insert duplicate rows
    n_dup = int(len(df_dirty) * dup_ratio)
    duplicates = df_dirty.sample(n_dup, random_state=42)
    df_dirty = pd.concat([df_dirty, duplicates], ignore_index=True)

    #  2. Insert missing values (randomly in selected feature columns)
    n_missing = int(df_dirty.size * missing_ratio)
    for _ in range(n_missing):
        row_idx = random.randint(0, df_dirty.shape[0] - 1)
        col_idx = random.randint(2, df_dirty.shape[1] - 1)  # Skip 'subject' and 'activity' columns
        df_dirty.iat[row_idx, col_idx] = np.nan

    print(f"[DIRTY] Number of duplicate rows inserted: {n_dup}")
    print(f"[DIRTY] Number of missing value cells inserted: {n_missing}")
    return df_dirty

# Generate dirty dataset
df_dirty = inject_dirty_data(df_selected)

print(f"[DIRTY] Total number of samples generated: {df_dirty.shape[0]}")
print(f"[DIRTY] Number of samples with missing values: {df_dirty.isnull().any(axis=1).sum()}")
print(f"[DIRTY] Are there any duplicates?: {df_dirty.duplicated().any()}")

"""### 1-3. Data Scaling (StandardScaler)"""

from sklearn.preprocessing import StandardScaler

# Before scaling: retain source
df_model = df_dirty.copy()

# Scaling target features
X = df_model[selected_features]

# Applying Scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Scaling and converting to data frames
X_scaled_df = pd.DataFrame(X_scaled, columns=selected_features)

# Combine with existing subject, activity
df_scaled = pd.concat([df_model[["subject", "activity"]].reset_index(drop=True), X_scaled_df], axis=1)

print(f"[SCALE] 스케일링 완료. shape: {df_scaled.shape}")
df_scaled.head()

"""### 1-4. Label Encoding"""

from sklearn.preprocessing import LabelEncoder

# Create a copy
df_labeled = df_scaled.copy()

# Perform label encoding
le = LabelEncoder()
df_labeled['activity_label'] = le.fit_transform(df_labeled['activity'])

# Class mapping output
activity_mapping = dict(zip(le.classes_, le.transform(le.classes_)))
print("[LABEL] 클래스 매핑:", activity_mapping)

# Isolation of data for final learning
X_final = df_labeled[selected_features]
y_final = df_labeled['activity_label']

from sklearn.preprocessing import StandardScaler, LabelEncoder
import pandas as pd

def preprocess_classification_data(df_dirty, selected_features):
    """
    Preprocessing function for training classification models
    - Remove duplicates
    - Drop missing values
    - Apply StandardScaler for feature scaling
    - Encode activity labels
    - Return final X, y for training

    Parameters:
        df_dirty (pd.DataFrame): Original dirty dataset
        selected_features (List[str]): List of feature column names to use

    Returns:
        X_final (pd.DataFrame): Preprocessed feature data
        y_final (pd.Series): Encoded labels
        df_scaled (pd.DataFrame): Scaled data including subject and activity
        activity_mapping (dict): Mapping information of encoded classes
    """
    #  1. Remove duplicates
    df_cleaned = df_dirty.drop_duplicates()

    #  2. Drop missing values
    df_cleaned = df_cleaned.dropna()

    print(f"[CLEAN] Number of samples after cleaning: {df_cleaned.shape[0]}")
    print(f"[CLEAN] Remaining missing values: {df_cleaned.isnull().sum().sum()}")
    print(f"[CLEAN] Any duplicates remaining?: {df_cleaned.duplicated().any()}")

    #  3. Feature scaling
    df_model = df_cleaned.copy()
    X = df_model[selected_features]
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    X_scaled_df = pd.DataFrame(X_scaled, columns=selected_features)

    df_scaled = pd.concat([df_model[["subject", "activity"]].reset_index(drop=True), X_scaled_df], axis=1)
    print(f"[SCALE] Scaling complete. Shape: {df_scaled.shape}")

    #  4. Label encoding
    df_labeled = df_scaled.copy()
    le = LabelEncoder()
    df_labeled["activity_label"] = le.fit_transform(df_labeled["activity"])
    activity_mapping = dict(zip(le.classes_, le.transform(le.classes_)))
    print("[LABEL] Class mapping:", activity_mapping)

    #  5. Split final data
    X_final = df_labeled[selected_features]
    y_final = df_labeled["activity_label"]

    return X_final, y_final, df_scaled, activity_mapping

"""### 1-5. Classification model learning"""

import numpy as np
import pandas as pd
import random
import matplotlib.pyplot as plt

from sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report


def run_classification_pipeline(df_dirty, selected_features, activity_labels):
    """
    Full pipeline for classification
    - Clean dirty data (remove duplicates, missing)
    - Encode labels
    - Train multiple ML models with CV
    - Visualize performance
    - Report final result using MLP

    Parameters:
        df_dirty (pd.DataFrame): Dataset with noise (missing values, duplicates)
        selected_features (List[str]): Selected feature columns
        activity_labels (List[str]): List of label names in order

    Returns:
        report (dict): Classification report dictionary
    """

    #  Step 1: Preprocessing
    def preprocess_classification_data(df_dirty, selected_features):
        """
        Preprocessing function for training classification models
        - Remove duplicates
        - Drop missing values
        - Apply StandardScaler for feature scaling
        - Encode activity labels
        - Return final X, y for training
        """
        df_clean = df_dirty.drop_duplicates().dropna()
        print(f"[CLEAN] Number of samples after cleaning: {df_clean.shape[0]}")
        print(f"[CLEAN] Remaining missing values: {df_clean.isnull().sum().sum()}")
        print(f"[CLEAN] Any duplicates remaining?: {df_clean.duplicated().any()}")

        scaler = StandardScaler()
        X = df_clean[selected_features]
        X_scaled = scaler.fit_transform(X)
        df_scaled = df_clean.copy()
        df_scaled[selected_features] = X_scaled

        label_map = {label: idx for idx, label in enumerate(sorted(df_clean['activity'].unique()))}
        print(f"[LABEL] Class mapping: {label_map}")
        y = df_clean['activity'].map(label_map)

        return X_scaled, y, df_scaled, label_map

    #  Step 2: Train models with CV
    def evaluate_models(X, y):
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        results = {}

        # 1. MLP
        mlp = MLPClassifier(hidden_layer_sizes=(100,), solver='adam', max_iter=300, random_state=42)
        results["MLP (NN)"] = cross_val_score(mlp, X, y, cv=cv, scoring='accuracy')

        # 2. Decision Trees
        for depth in [2, 20, 100]:
            dt = DecisionTreeClassifier(max_depth=depth, random_state=42)
            results[f"Decision Tree (depth={depth})"] = cross_val_score(dt, X, y, cv=cv, scoring='accuracy')

        # 3. Naive Bayes
        nb = GaussianNB()
        results["Naive Bayes (GaussianNB)"] = cross_val_score(nb, X, y, cv=cv, scoring='accuracy')

        # 4. SVM
        svm = SVC(kernel='poly', degree=3, C=1.0)
        results["SVM (poly, deg=3)"] = cross_val_score(svm, X, y, cv=cv, scoring='accuracy')

        # 5. k-NN
        for k in [1, 3]:
            knn = KNeighborsClassifier(n_neighbors=k)
            results[f"k-NN (k={k})"] = cross_val_score(knn, X, y, cv=cv, scoring='accuracy')

        return results

    #  Step 3: Visualize results
    def visualize_model_results(model_scores):
        model_names = list(model_scores.keys())
        avg_scores = [np.mean(scores) for scores in model_scores.values()]
        std_scores = [np.std(scores) for scores in model_scores.values()]
        best_model_idx = int(np.argmax(avg_scores))
        best_model_name = model_names[best_model_idx]

        plt.figure(figsize=(12, 6))
        plt.barh(model_names, avg_scores, xerr=std_scores, color='lightgreen')
        plt.xlabel("Average Accuracy (5-Fold CV)")
        plt.title("Model Performance Comparison")
        plt.axvline(x=avg_scores[best_model_idx], color='red', linestyle='--', label=f'Best: {best_model_name}')
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.gca().invert_yaxis()
        plt.show()

        return best_model_name, avg_scores[best_model_idx]

    #  Step 4: Final evaluation
    def report_on_test_split(X_final, y_final, activity_labels):
        X_train, X_test, y_train, y_test = train_test_split(
            X_final, y_final, test_size=0.2, stratify=y_final, random_state=42
        )
        mlp = MLPClassifier(hidden_layer_sizes=(100,), solver='adam', max_iter=300, random_state=42)
        mlp.fit(X_train, y_train)
        y_pred = mlp.predict(X_test)

        report = classification_report(y_test, y_pred, target_names=activity_labels, output_dict=True)
        return report

    #  Pipeline
    X_final, y_final, df_scaled, activity_mapping = preprocess_classification_data(df_dirty, selected_features)
    model_scores = evaluate_models(X_final, y_final)

    print("\n5-Fold Cross-Validation Accuracy Comparison\n")
    for name, scores in model_scores.items():
        print(f"{name}: Mean Accuracy = {np.mean(scores):.4f}, Std Dev = {np.std(scores):.4f}")

    best_model_name, best_score = visualize_model_results(model_scores)
    print(f"\nBest Model: {best_model_name} with Accuracy = {best_score:.4f}")

    report = report_on_test_split(X_final, y_final, activity_labels)
    return report

activity_labels = ["WALKING", "WALKING_UPSTAIRS", "WALKING_DOWNSTAIRS", "SITTING", "STANDING", "LAYING"]
report = run_classification_pipeline(df_dirty, selected_features, activity_labels)

pd.DataFrame(report).transpose()

"""# 2. Regression"""

from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.preprocessing import OneHotEncoder
import pandas as pd
import numpy as np

def preprocess_regression_data(df_dirty, selected_features):
    """
    Preprocessing function for training regression-based classification models
    - Remove duplicates
    - Drop missing values
    - Apply StandardScaler for feature scaling
    - One-hot encode activity labels
    - Return final X, y for regression model training

    Parameters:
        df_dirty (pd.DataFrame): Original dirty dataset with noise
        selected_features (List[str]): List of feature column names to use

    Returns:
        X_final (pd.DataFrame): Scaled feature data
        y_onehot (np.ndarray): One-hot encoded activity labels
        df_scaled (pd.DataFrame): DataFrame with scaled features + labels
        label_encoder (LabelEncoder): LabelEncoder used for inverse_transform
        onehot_encoder (OneHotEncoder): OneHotEncoder used for decoding
    """
    #  1. Remove duplicates
    df_cleaned = df_dirty.drop_duplicates()

    #  2. Drop missing values
    df_cleaned = df_cleaned.dropna()

    print(f"[CLEAN] Number of samples after cleaning: {df_cleaned.shape[0]}")
    print(f"[CLEAN] Remaining missing values: {df_cleaned.isnull().sum().sum()}")
    print(f"[CLEAN] Any duplicates remaining?: {df_cleaned.duplicated().any()}")

    #  3. Feature scaling
    df_model = df_cleaned.copy()
    X = df_model[selected_features]
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    X_scaled_df = pd.DataFrame(X_scaled, columns=selected_features)

    #  4. Label encoding → One-hot encoding
    le = LabelEncoder()
    y_labels = le.fit_transform(df_model["activity"])
    ohe = OneHotEncoder(sparse_output=False)
    y_onehot = ohe.fit_transform(y_labels.reshape(-1, 1))

    print(f"[LABEL] Label classes: {list(le.classes_)}")
    print(f"[LABEL] One-hot encoded shape: {y_onehot.shape}")

    #  5. Final outputs
    df_scaled = pd.concat([df_model[["subject", "activity"]].reset_index(drop=True), X_scaled_df], axis=1)
    X_final = X_scaled_df

    return X_final, y_onehot, df_scaled, le, ohe

# Updated full regression pipeline using preprocessed X and y_onehot (no preprocessing inside)

def run_regression_evaluation_from_preprocessed(X, y_onehot, activity_labels):
    """
    Run regression evaluation using preprocessed data.
    Includes:
    - Accuracy/MSE/Cosine Similarity per model
    - Visualization for top 5 models
    - Per-activity classification report for best model

    Parameters:
        X (np.ndarray): Scaled feature data
        y_onehot (np.ndarray): One-hot encoded labels
        activity_labels (List[str]): Original class label names

    Returns:
        df_results (pd.DataFrame): Model-level evaluation summary
        df_activity_report (pd.DataFrame): Per-activity classification report (best model)
    """
    from sklearn.linear_model import Ridge, Lasso
    from sklearn.neighbors import KNeighborsRegressor
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.neural_network import MLPRegressor
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import mean_squared_error, accuracy_score
    from sklearn.metrics.pairwise import cosine_similarity
    import matplotlib.pyplot as plt
    import seaborn as sns
    import pandas as pd
    import numpy as np

    X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)

    models = {
        "Ridge (alpha=1.0)": Ridge(alpha=1.0),
        "Ridge (alpha=10.0)": Ridge(alpha=10.0),
        "Lasso (alpha=0.01)": Lasso(alpha=0.01),
        "Lasso (alpha=0.1)": Lasso(alpha=0.1),
        "KNN (k=3)": KNeighborsRegressor(n_neighbors=3),
        "KNN (k=7)": KNeighborsRegressor(n_neighbors=7),
        "Random Forest (100 trees)": RandomForestRegressor(n_estimators=100, random_state=42),
        "MLP (50 hidden)": MLPRegressor(hidden_layer_sizes=(50,), max_iter=500, random_state=42),
        "MLP (100 hidden)": MLPRegressor(hidden_layer_sizes=(100,), max_iter=500, random_state=42),
        "MLP (100-50 hidden)": MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)
    }

    results = []
    predictions = {}

    for name, model in models.items():
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        y_pred_labels = np.argmax(y_pred, axis=1)
        y_true_labels = np.argmax(y_test, axis=1)

        accuracy = accuracy_score(y_true_labels, y_pred_labels)
        mse = mean_squared_error(y_test, y_pred)
        cos_sim = np.mean([
            cosine_similarity(y_test[i].reshape(1, -1), y_pred[i].reshape(1, -1))[0, 0]
            for i in range(len(y_test))
        ])

        results.append({
            "Model": name,
            "Accuracy": accuracy,
            "MSE": mse,
            "Cosine Similarity": cos_sim
        })

        predictions[name] = (y_true_labels, y_pred_labels)

    df_results = pd.DataFrame(results).sort_values("Accuracy", ascending=False)

    # Accuracy Bar Plot
    plt.figure(figsize=(10, 6))
    plt.barh(df_results["Model"], df_results["Accuracy"], color="skyblue")
    plt.xlabel("Accuracy (Argmax of One-Hot Prediction)")
    plt.title("Regression Model Comparison")
    plt.grid(True, axis="x", linestyle="--", alpha=0.7)
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.show()


    # Best Model: Classification Report
    best_model_name = df_results.iloc[0]["Model"]
    print(f"\n Best Model: {best_model_name}")
    best_true, best_pred = predictions[best_model_name]
    report = classification_report(best_true, best_pred, target_names=activity_labels, output_dict=True)
    df_activity_report = pd.DataFrame(report).transpose()

    return df_results, df_activity_report


activity_labels = ["WALKING", "WALKING_UPSTAIRS", "WALKING_DOWNSTAIRS", "SITTING", "STANDING", "LAYING"]

X_final, y_onehot, df_scaled, le, ohe = preprocess_regression_data(df_dirty, selected_features)
final_report, activity_report = run_regression_evaluation_from_preprocessed(X_final, y_onehot, activity_labels)

print("\n 모델별 종합 성능:\n", final_report)
print("\n 활동별 예측 성능:\n", activity_report)